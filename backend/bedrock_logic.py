import boto3
import json
import os
import requests
import re
from bs4 import BeautifulSoup
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from sqlalchemy.orm import Session
from datetime import date, datetime, timedelta

# --- ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë“ˆ ì„í¬íŠ¸ ---
# ai_challenge_routerì—ì„œ ì§ì ‘ í•¨ìˆ˜ì™€ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from backend.routes.ai_challenge_router import AICallengeCreateRequest, create_and_join_ai_challenge
from backend.routes.dashboard import get_dashboard
from backend import crud, models, schemas
from backend.models import User, TransportMode, Challenge, ChallengeMember
from backend.database import get_db

# --- ì„¤ì • ---
AWS_DEFAULT_REGION = "us-east-1"
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyBgs37kJYWB7zsTfIrDTqe1hpOxBhNkH44")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "01354cc88406341ec")
BEDROCK_MODEL_ARN = os.getenv("BEDROCK_MODEL_ARN", "arn:aws:bedrock:us-east-1:327784329358:inference-profile/us.anthropic.claude-opus-4-20250514-v1:0")
BEDROCK_KNOWLEDGE_BASE_ID = os.getenv("BEDROCK_KNOWLEDGE_BASE_ID", "PUGB1AL6L1")

# --- Boto3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    bedrock_runtime_client = boto3.client('bedrock-runtime', region_name=AWS_DEFAULT_REGION)
    bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=AWS_DEFAULT_REGION)
    print("[ì•Œë¦¼] AWS Bedrock í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"[ì˜¤ë¥˜] AWS í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    bedrock_runtime_client = None
    bedrock_agent_runtime_client = None

# FastAPI ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/chat",
    tags=["Chatbot"]
)

class ChatRequest(BaseModel):
    user_id: int
    message: str

def invoke_llm(system_prompt, user_prompt):
    if not bedrock_runtime_client:
        raise ConnectionError("Bedrock runtime client is not initialized.")
    try:
        messages = [{"role": "user", "content": user_prompt}]
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4096,
            "system": system_prompt,
            "messages": messages
        }
        response = bedrock_runtime_client.invoke_model(modelId=BEDROCK_MODEL_ARN, body=json.dumps(request_body))
        response_body = json.loads(response.get('body').read())
        return response_body['content'][0]['text']
    except Exception as e:
        print(f"Bedrock ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def query_knowledge_base(query):
    if not bedrock_agent_runtime_client:
        raise ConnectionError("Bedrock agent runtime client is not initialized.")
    print(f"\n[ì•Œë¦¼] Bedrock ì§€ì‹ ê¸°ë°˜ì—ì„œ '{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    try:
        response = bedrock_agent_runtime_client.retrieve_and_generate(
            input={'text': query},
            retrieveAndGenerateConfiguration={
                'type': 'KNOWLEDGE_BASE',
                'knowledgeBaseConfiguration': {'knowledgeBaseId': BEDROCK_KNOWLEDGE_BASE_ID, 'modelArn': BEDROCK_MODEL_ARN}
            }
        )
        if response and response.get('output') and response.get('citations'):
            answer = response['output']['text']
            citations = response['citations']
            source_details = []
            for citation in citations:
                if citation.get('retrievedReferences'):
                    retrieved_ref = citation['retrievedReferences'][0]
                    location = retrieved_ref.get('location', {}).get('s3Location', {}).get('uri')
                    if location:
                        source_details.append(f"- {location}")
            formatted_answer = f"{answer}\n\n--- ì¶œì²˜ ---\n" + "\n".join(source_details) if source_details else answer
            print("[ì•Œë¦¼] ì§€ì‹ ê¸°ë°˜ì—ì„œ ë‹µë³€ì„ ì„±ê³µì ìœ¼ë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return formatted_answer
        else:
            print("[ì•Œë¦¼] ì§€ì‹ ê¸°ë°˜ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        print(f"Bedrock ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

def perform_web_search(query):
    print(f"\n[ì•Œë¦¼] ì›¹ì—ì„œ '{query}'ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    try:
        search_url = "https://www.googleapis.com/customsearch/v1"
        search_params = {'key': GOOGLE_API_KEY, 'cx': GOOGLE_CSE_ID, 'q': query, 'num': 3}
        search_response = requests.get(search_url, params=search_params)
        search_response.raise_for_status()
        search_results = search_response.json()
        items = search_results.get('items', [])
        if not items: return "ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        full_context = ""
        urls = [item.get('link') for item in items]
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        print("[ì•Œë¦¼] ê²€ìƒ‰ëœ ì›¹í˜ì´ì§€ì˜ ë³¸ë¬¸ì„ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤...")
        for url in urls:
            if not url: continue
            try:
                page_response = requests.get(url, headers=headers, timeout=5)
                page_response.raise_for_status()
                soup = BeautifulSoup(page_response.text, 'lxml')
                text_parts = [element.get_text(strip=True) for tag in ['h1', 'h2', 'h3', 'p'] for element in soup.find_all(tag)]
                page_text = '\n'.join(text_parts)
                full_context += f"--- URL: {url}ì˜ ë‚´ìš© ---\n{page_text}\n\n"
            except requests.exceptions.RequestException as e:
                print(f"  - URL {url} ë°©ë¬¸ ì‹¤íŒ¨: {e}")
                continue
        if not full_context: return "ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        print(f"[ì¶”ì¶œ ì™„ë£Œ] ì´ {len(full_context)}ìë¦¬ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        return full_context
    except Exception as e:
        print(f"ì›¹ ê²€ìƒ‰ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return "ì •ë³´ ê²€ìƒ‰ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

async def _handle_recommend_challenge(user_query: str, user_id: int, db: Session, router_decision: dict):
    """AI ì±Œë¦°ì§€ ì¶”ì²œ ë° ìƒì„± ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    print("[ì•Œë¦¼] AI ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.")
    current_user_obj = db.query(User).filter(User.user_id == user_id).first()
    if not current_user_obj:
        raise HTTPException(status_code=404, detail="User not found for challenge recommendation.")
    
    dashboard_data = await get_dashboard(current_user=current_user_obj, db=db)
    
    challenge_prompt = f"""
    You are an AI assistant that generates eco-friendly challenge ideas.
    Based on the user's intent and their recent activity data, generate a single challenge idea in JSON format.
    The challenge should be simple, actionable, and encourage carbon reduction.
    Prioritize light challenges that the user hasn't done much recently, or suggest new types of activities.
    Avoid recommending challenges for activities the user has frequently done in the last 7 days.
    
    User's recent activity data:
    - Last 7 days carbon saved (g): {json.dumps([{"date": str(d.date), "saved_g": d.saved_g} for d in dashboard_data.last7days])}
    - Mode statistics: {json.dumps([{"mode": m.mode, "saved_g": m.saved_g} for m in dashboard_data.modeStats])}
    - Total carbon saved (kg): {dashboard_data.total_saved}
    - Current garden level: {dashboard_data.garden_level}
    
    Provide a title, a short description, a reward (integer), a goal_type (must be one of: CO2_SAVED, DISTANCE_KM, TRIP_COUNT), a goal_target_value (float), and an optional target_mode (ANY, WALK, BIKE, PUBLIC_TRANSPORT).
    
    Example for a distance-based challenge:
    {{
        "title": "ì£¼ë§ì— 3km ê±·ê¸°",
        "description": "ì´ë²ˆ ì£¼ë§, ì°¨ ëŒ€ì‹  ë‘ ë°œë¡œ 3kmë¥¼ ê±¸ì–´ë³´ì„¸ìš”!",
        "reward": 50,
        "target_mode": "WALK",
        "goal_type": "DISTANCE_KM",
        "goal_target_value": 3.0
    }}
    Example for a general CO2-based challenge:
    {{
        "title": "ë¶„ë¦¬ìˆ˜ê±° ì±Œë¦°ì§€",
        "description": "ì˜¤ëŠ˜ í•˜ë£¨ ë¶„ë¦¬ìˆ˜ê±°ë¥¼ ì™„ë²½í•˜ê²Œ ì‹¤ì²œí•´ì„œ íƒ„ì†Œ ë°°ì¶œì„ ì¤„ì—¬ë³´ì„¸ìš”!",
        "reward": 20,
        "target_mode": "ANY",
        "goal_type": "CO2_SAVED",
        "goal_target_value": 100.0
    }}
    
    User intent: "{router_decision.get("user_intent", user_query)}"
    Your JSON response:
    """
    
    challenge_idea_str = invoke_llm(challenge_prompt, "")
    
    try:
        challenge_idea = json.loads(challenge_idea_str)
        
        challenge_request = AICallengeCreateRequest(
            title=challenge_idea["title"],
            description=challenge_idea["description"],
            reward=challenge_idea["reward"],
            target_mode=TransportMode[challenge_idea.get("target_mode", "ANY").upper()],
            goal_type=schemas.ChallengeGoalType[challenge_idea["goal_type"].upper()],
            goal_target_value=float(challenge_idea["goal_target_value"])
        )
        
        challenge_response = await create_and_join_ai_challenge(
            request=challenge_request,
            db=db,
            current_user=current_user_obj
        )
        
        final_answer = challenge_response.get("message", "AI ì±Œë¦°ì§€ ìƒì„± ë° ì°¸ì—¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        if challenge_response.get("challenge"):
            final_answer += f" ì±Œë¦°ì§€ ì œëª©: {challenge_response['challenge'].title}"
        return final_answer

    except (json.JSONDecodeError, KeyError) as e:
        print(f"[ì˜¤ë¥˜] AI ì±Œë¦°ì§€ ì•„ì´ë””ì–´ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "AI ì±Œë¦°ì§€ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì œì•ˆí•´ë³¼ê¹Œìš”?"
    except Exception as e:
        print(f"[ì˜¤ë¥˜] AI ì±Œë¦°ì§€ ìƒì„± ë° ì°¸ì—¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"AI ì±Œë¦°ì§€ ìƒì„± ë° ì°¸ì—¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

@router.post("/")
async def chatbot_endpoint(request: ChatRequest, db: Session = Depends(get_db)):
    user_query = request.message
    user_id = request.user_id

    print(f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n")
    print("[1ë‹¨ê³„] ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤...")
    router_system_prompt = f"""
    You are a smart orchestrator that analyzes the user's question and decides which action to take... 
    (ì´í•˜ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ê¸°ì¡´ê³¼ ë™ì¼)
    """
    
    router_output_str = invoke_llm(router_system_prompt, user_query)
    
    action, router_decision = None, {}
    if not router_output_str:
        action, router_decision = "general_search", {"query": user_query}
    else:
        try:
            json_match = re.search(r'\{.*\}', router_output_str, re.DOTALL)
            if json_match:
                router_decision = json.loads(json_match.group())
                action = router_decision.get("action")
            else: raise ValueError("No JSON object found")
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ì¡°ìœ¨ì(Router) ê²°ì • íŒŒì‹± ì‹¤íŒ¨: {e}. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            action, router_decision = "general_search", {"query": user_query}

    final_answer, query, original_action = "", router_decision.get("query", user_query), action

    if action == "knowledge_base_search":
        print(f"[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: '{action}'. ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        final_answer = query_knowledge_base(query)
        if not final_answer:
            print("[ì•Œë¦¼] ì§€ì‹ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            action = "general_search"

    if action == "general_search":
        if original_action != 'knowledge_base_search':
            print(f"[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: '{action}'. ì›¹ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        search_results = perform_web_search(query)
        if "ì˜¤ë¥˜" in search_results or "ì—†ìŠµë‹ˆë‹¤" in search_results:
            final_answer = search_results
        else:
            search_results = search_results[:20000]
            print("\n[3ë‹¨ê³„] ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            final_answer_system_prompt = "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´... (ì´í•˜ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ê¸°ì¡´ê³¼ ë™ì¼)"
            final_answer = invoke_llm(final_answer_system_prompt, f"<search_results>\n{search_results}\n</search_results>\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_query}")

    elif action == "detect_activity_and_suggest_challenge":
        print("[ì•Œë¦¼] ì¡°ìœ¨ì íŒë‹¨: 'detect_activity_and_suggest_challenge'.")
        activity_keywords = {"ìì „ê±°": TransportMode.BIKE, "ê±¸ì–´ì„œ": TransportMode.WALK, "ë„ë³´": TransportMode.WALK, "ë²„ìŠ¤": TransportMode.BUS, "ì§€í•˜ì² ": TransportMode.SUBWAY}
        detected_keyword, detected_activity_mode = None, None
        for keyword, mode in activity_keywords.items():
            if keyword in user_query:
                detected_keyword, detected_activity_mode = keyword, mode
                break
        
        if not detected_activity_mode:
            final_answer = invoke_llm("You are a friendly AI assistant.", user_query)
        else:
            utc_today_start = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(hours=9)
            if datetime.utcnow().hour < 9: utc_today_start -= timedelta(days=1)
            utc_today_end = utc_today_start + timedelta(days=1)
            mobility_log = db.query(models.MobilityLog).filter(models.MobilityLog.user_id == user_id, models.MobilityLog.transport_mode == detected_activity_mode, models.MobilityLog.started_at >= utc_today_start, models.MobilityLog.started_at < utc_today_end).order_by(models.MobilityLog.started_at.desc()).first()
            if mobility_log:
                bonus_credits = int(mobility_log.distance_km * 5)
                crud.create_credit_log(db, user_id=user_id, points=bonus_credits, reason=f"ì±—ë´‡ í™œë™ í™•ì¸ ë³´ë„ˆìŠ¤: {detected_keyword}")
                final_answer = f"ë„¤! ì˜¤ëŠ˜ {mobility_log.distance_km:.1f}kmë¥¼ {detected_keyword}(ìœ¼)ë¡œ ì´ë™í•˜ì‹  ê¸°ë¡ì„ í™•ì¸í–ˆì–´ìš”. ì •ë§ ë©‹ì ¸ìš”! ì¶”ê°€ ë³´ë„ˆìŠ¤ë¡œ {bonus_credits}Cë¥¼ ë“œë ¸ìŠµë‹ˆë‹¤. ğŸ"
            else:
                joined_challenge_ids = {m.challenge_id for m in db.query(models.ChallengeMember).filter(models.ChallengeMember.user_id == user_id).all()}
                available_challenges = db.query(models.Challenge).filter(models.Challenge.challenge_id.notin_(joined_challenge_ids), models.Challenge.title.contains(detected_keyword)).all()
                if available_challenges:
                    suggested_challenge = available_challenges[0]
                    suggestion_prompt = f"You are a friendly and encouraging AI assistant... (ì´í•˜ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ê¸°ì¡´ê³¼ ë™ì¼)"
                    final_answer = invoke_llm(suggestion_prompt, "")
                else:
                    print(f"[ì•Œë¦¼] ê´€ë ¨ ì±Œë¦°ì§€ ì—†ìŒ. ì‚¬ìš©ì ë§ì¶¤í˜• AI ì±Œë¦°ì§€ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                    final_answer = await _handle_recommend_challenge(user_query, user_id, db, {"user_intent": f"{detected_keyword} íƒ€ê¸°ì™€ ê´€ë ¨ëœ ì±Œë¦°ì§€ ì¶”ì²œ"})

    elif action == "recommend_challenge":
        final_answer = await _handle_recommend_challenge(user_query, user_id, db, router_decision)

    elif action in ["get_carbon_reduction_tip", "get_goal_strategy"]:
        final_answer = invoke_llm("You are a helpful AI assistant...", router_decision.get("user_intent", user_query))

    elif action == "direct_answer":
        final_answer = router_decision.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if not final_answer:
        final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    print("\n--- ìµœì¢… ë‹µë³€ ---\n" + final_answer)
    return {"response": final_answer}